{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahashikoji/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/takahashikoji/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/takahashikoji/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'tra': pd.read_csv('air_visit_data.csv'),\n",
    "    'as': pd.read_csv('air_store_info.csv'),\n",
    "    'hs': pd.read_csv('hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('air_reserve.csv'),\n",
    "    'hr': pd.read_csv('hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('sample_submission.csv'),\n",
    "    'hol': pd.read_csv('date_info.csv').rename(columns={'calendar_date': 'visit_date'})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# リレーションを使ってair_store_id と共通のhpg_store_id　だけを残している。\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(4):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name' +str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id','air_store_id','visit_date','visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(train[col])\n",
    "y = np.log1p(train['visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahashikoji/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "for train_index,test_index in skf.split(X,y):\n",
    "    X_train,X_test = X[train_index],X[test_index]\n",
    "    y_train,y_test = y[train_index],y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589966776669 {'max_depth': 5, 'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xreg =  XGBRegressor()\n",
    "\n",
    "params = {'max_depth':[5,6,7,8],\n",
    "         'n_estimators':[70,80,90,100,120]}\n",
    "\n",
    "clf = GridSearchCV(xreg , params)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "print(clf.best_score_,clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=70, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRegressor(**clf.best_params_)\n",
    "\n",
    "xgb.fit(X_train , y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49821585540220686"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    return metrics.mean_squared_error(y_true, y_pred)**0.5\n",
    "\n",
    "RMSLE(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  4.46946208,   4.82408396,   5.67184591,   6.32068809,\n",
       "          7.34450698,   5.47093534,   5.87609132,   6.32691296,\n",
       "          6.90483928,   7.74109705,   6.42832041,   6.98489205,\n",
       "          8.08523711,   8.87524589,  10.02005378,   7.27234324,\n",
       "          8.22932371,   9.0779167 ,   9.46051145,  10.93361036]),\n",
       " 'mean_score_time': array([ 0.07303929,  0.10788441,  0.09163205,  0.09907524,  0.10366201,\n",
       "         0.08755334,  0.09415436,  0.10000181,  0.10079757,  0.10979692,\n",
       "         0.11478933,  0.11914595,  0.13039581,  0.11102287,  0.12665057,\n",
       "         0.09927797,  0.11314885,  0.12458396,  0.13024569,  0.15229885]),\n",
       " 'mean_test_score': array([ 0.58996678,  0.58994046,  0.58987541,  0.5894505 ,  0.5884477 ,\n",
       "         0.58866447,  0.58822725,  0.58815211,  0.58813308,  0.58812566,\n",
       "         0.58722102,  0.5872513 ,  0.58719607,  0.58717085,  0.5871533 ,\n",
       "         0.58653818,  0.58657773,  0.58653038,  0.58648242,  0.58646016]),\n",
       " 'mean_train_score': array([ 0.61544369,  0.61781601,  0.61987481,  0.62147587,  0.62362729,\n",
       "         0.62654422,  0.62796056,  0.6284266 ,  0.62843821,  0.62844282,\n",
       "         0.63987336,  0.64132828,  0.64242098,  0.64336388,  0.64340022,\n",
       "         0.65519008,  0.65668961,  0.6577698 ,  0.65778737,  0.65779332]),\n",
       " 'param_max_depth': masked_array(data = [5 5 5 5 5 6 6 6 6 6 7 7 7 7 7 8 8 8 8 8],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [70 80 90 100 120 70 80 90 100 120 70 80 90 100 120 70 80 90 100 120],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'max_depth': 5, 'n_estimators': 70},\n",
       "  {'max_depth': 5, 'n_estimators': 80},\n",
       "  {'max_depth': 5, 'n_estimators': 90},\n",
       "  {'max_depth': 5, 'n_estimators': 100},\n",
       "  {'max_depth': 5, 'n_estimators': 120},\n",
       "  {'max_depth': 6, 'n_estimators': 70},\n",
       "  {'max_depth': 6, 'n_estimators': 80},\n",
       "  {'max_depth': 6, 'n_estimators': 90},\n",
       "  {'max_depth': 6, 'n_estimators': 100},\n",
       "  {'max_depth': 6, 'n_estimators': 120},\n",
       "  {'max_depth': 7, 'n_estimators': 70},\n",
       "  {'max_depth': 7, 'n_estimators': 80},\n",
       "  {'max_depth': 7, 'n_estimators': 90},\n",
       "  {'max_depth': 7, 'n_estimators': 100},\n",
       "  {'max_depth': 7, 'n_estimators': 120},\n",
       "  {'max_depth': 8, 'n_estimators': 70},\n",
       "  {'max_depth': 8, 'n_estimators': 80},\n",
       "  {'max_depth': 8, 'n_estimators': 90},\n",
       "  {'max_depth': 8, 'n_estimators': 100},\n",
       "  {'max_depth': 8, 'n_estimators': 120}],\n",
       " 'rank_test_score': array([ 1,  2,  3,  4,  6,  5,  7,  8,  9, 10, 12, 11, 13, 14, 15, 17, 16,\n",
       "        18, 19, 20], dtype=int32),\n",
       " 'split0_test_score': array([ 0.57883042,  0.57859791,  0.57830815,  0.57760258,  0.57694087,\n",
       "         0.57584079,  0.57478077,  0.57466935,  0.57466382,  0.57466119,\n",
       "         0.57258518,  0.57278361,  0.5726476 ,  0.57258841,  0.5725401 ,\n",
       "         0.57065556,  0.57076113,  0.57061938,  0.57047621,  0.5704232 ]),\n",
       " 'split0_train_score': array([ 0.61685173,  0.61960501,  0.62136265,  0.62319488,  0.62585191,\n",
       "         0.62780506,  0.63076762,  0.63206609,  0.63206613,  0.63206615,\n",
       "         0.64183323,  0.64564123,  0.64888408,  0.65170216,  0.6518071 ,\n",
       "         0.65911427,  0.66356814,  0.66679759,  0.66684722,  0.66686376]),\n",
       " 'split1_test_score': array([ 0.5867683 ,  0.58671765,  0.58634569,  0.58586835,  0.5838035 ,\n",
       "         0.58528349,  0.58520402,  0.58519482,  0.58519198,  0.58519338,\n",
       "         0.58521508,  0.58521875,  0.58523706,  0.58524944,  0.58525879,\n",
       "         0.58486741,  0.58493473,  0.58495453,  0.58496026,  0.5849522 ]),\n",
       " 'split1_train_score': array([ 0.61754577,  0.61968344,  0.62183094,  0.62353619,  0.62716661,\n",
       "         0.62841159,  0.62955674,  0.62962884,  0.62965697,  0.62966832,\n",
       "         0.64278807,  0.64330884,  0.64333538,  0.64334364,  0.64334678,\n",
       "         0.65831289,  0.65834401,  0.65835215,  0.65835444,  0.65835546]),\n",
       " 'split2_test_score': array([ 0.60430187,  0.60450607,  0.60497266,  0.60488086,  0.604599  ,\n",
       "         0.60486942,  0.60469727,  0.60459244,  0.60454372,  0.60452271,\n",
       "         0.60386311,  0.60375183,  0.60370386,  0.60367498,  0.6036613 ,\n",
       "         0.60409189,  0.60403765,  0.60401754,  0.60401109,  0.60400539]),\n",
       " 'split2_train_score': array([ 0.61193356,  0.61415958,  0.61643084,  0.61769654,  0.61786334,\n",
       "         0.62341603,  0.62355731,  0.62358488,  0.62359152,  0.62359398,\n",
       "         0.63499877,  0.63503477,  0.63504348,  0.63504583,  0.63504676,\n",
       "         0.64814307,  0.64815669,  0.64815966,  0.64816043,  0.64816073]),\n",
       " 'std_fit_time': array([ 0.35528329,  0.22644985,  0.56456875,  0.20599419,  0.29490019,\n",
       "         0.16736862,  0.13431321,  0.1962982 ,  0.08290376,  0.05421489,\n",
       "         0.18021414,  0.0567822 ,  0.4169689 ,  0.43274363,  0.6448088 ,\n",
       "         0.11900964,  0.24711307,  0.4154447 ,  0.23558893,  0.19591947]),\n",
       " 'std_score_time': array([ 0.00736853,  0.02591079,  0.01643549,  0.01144802,  0.00262322,\n",
       "         0.00307147,  0.01245914,  0.00426161,  0.01021403,  0.00203275,\n",
       "         0.02180004,  0.01046396,  0.02655524,  0.00634934,  0.01152265,\n",
       "         0.00158379,  0.00649752,  0.00740553,  0.00735608,  0.01239008]),\n",
       " 'std_test_score': array([ 0.01064177,  0.01081966,  0.01116819,  0.01142073,  0.01175922,\n",
       "         0.01208961,  0.01239902,  0.01239372,  0.01237441,  0.01236596,\n",
       "         0.01284768,  0.01272413,  0.01275409,  0.01276354,  0.01277559,\n",
       "         0.01370134,  0.01363465,  0.01368018,  0.01373278,  0.01375126]),\n",
       " 'std_train_score': array([ 0.00249815,  0.00258569,  0.00244275,  0.00267602,  0.00411091,\n",
       "         0.00222579,  0.00315257,  0.00356528,  0.00356546,  0.00356565,\n",
       "         0.00346883,  0.00455091,  0.00568728,  0.00679993,  0.00684248,\n",
       "         0.00499371,  0.00639953,  0.00762004,  0.00763938,  0.00764582])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross_val_score\n",
    "\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -0.02 (+/- 0.00)\n",
      "Fold    0: MSE= -0.0241389130517\n",
      "Fold    1: MSE= -0.0265881337388\n",
      "Fold    2: MSE= -0.02439166274\n",
      "Fold    3: MSE= -0.023680636589\n",
      "Fold    4: MSE= -0.0246996323316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scores = cross_val_score(xgb, X, y,scoring = \"neg_mean_squared_log_error\",cv=5)\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print('Fold %4d: MSE= %s'% (i, score))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
